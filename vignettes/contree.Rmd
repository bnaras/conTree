---
title: "Contrast and Boosted Trees"
author: "Jerome Friedman"
date: '`r Sys.Date()`'
output:
  html_document:
  fig_caption: yes
  theme: cerulean
  toc: yes
  toc_depth: 2
vignette: >
  %\VignetteIndexEntry{contree}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r echo=FALSE}
knitr::opts_chunk$set(
    message = FALSE,
    warning = FALSE,
    error = FALSE,
    tidy = FALSE,
    cache = FALSE
    )

## data generation
lgen <- function(n, f, s, dist) {
  if (dist == "logistic") {
    r <- runif(n)
    y <- f + s * log(r / (1 - r))
  }
  else if (dist == "normal") {
    y <- rnorm(n, f, 1.63107 * s)
  }
  else if (dist == "laplace") {
    y <- laplace(n, f, 1.585 * s)
  }
  else if (dist == "slash") {
    y <- f + 0.7522519 * s * rnorm(n) / runif(n)
  }
  else {
    stop("unsupported error distribution")
  }
  invisible(y)
}
datgen <- function(p = 10, no = 25000, scl = 0.5, sclf = 0.75, er = 2,
                   dist = "logistic", smax = 50) {
    x <- matrix(rnorm(p * no), ncol = p)
    b <- rnorm(p)
    b <- b / sqrt(sum(b**2))
    z <- er * runif(p)
    f <- rep(0, no)
    for (j in 1:p) {
      u <- sign(x[, j]) * abs(x[, j])**z[j]
      v <- sqrt(var(u))
      f <- f + b[j] * u / v
    }
    z <- er * runif(p)
    s <- rep(0, no)
    b <- rnorm(p)
    b <- b * sclf / sqrt(sum(b**2))
    for (j in 1:p) {
      u <- sign(x[, j]) * abs(x[, j])**z[j]
      v <- sqrt(var(u))
      s <- s + b[j] * u / v
    }
    s <- pmin(smax, scl * exp(s))
    y <- lgen(no, f, s, dist)
    invisible(list(x = x, y = y, f = f, s = s))
}

## predictions
lmpred <- function(x, lmm) {
  lmm$coefficients[1] + x %*% lmm$coefficients[2:length(lmm$coefficients)]
}

library("randomForest")

```

## Contrast demo


```{r}
library(contree)

## generate training and test data sets
## n=25000, p=10, heteroskedastic logistic error
set.seed(12345)
d <- datgen()
e <- datgen()

## fit random forest (RF) model on training data
rf <- randomForest::randomForest(d$x, d$y)
##
## RF predicted values on test data
##
erf <- predict(rf, e$x)

## RF average absolute error from truth (e$f) on test data
cat(sprintf("RF mean absolute error: %f\n", mean(abs(e$f - erf))))

##
## fit linear model (LM) on training data
##
lmm <- lm(d$y ~ d$x)
##
## evaluate LM on test data
##
elm <- lmpred(e$x, lmm)
##
## LM average absolute error from truth (e$f) on test data
##
cat(sprintf("LM mean absolute error: %f\n", mean(abs(e$f - elm))))

##
## contrast y with RF predictions on test data
##
treerf <- contrast(e$x, e$y, erf, type = "diffmean")
##
## summarize terminal nodes
##
nodesum(e$x, e$y, erf, treerf)


##
## graphical summary
##
nodeplots(e$x, e$y, erf, treerf)

##
# x-region definitions for all terminal nodes
#
treesum(treerf)
#
# contrast y with LM predictions on test data
#
treelm <- contrast(e$x, e$y, elm, type = "diffmean")
nodesum(e$x, e$y, elm, treelm)
nodeplots(e$x, e$y, elm, treelm)
#
# contrast y with true f(x) on test data
#
treef <- contrast(e$x, e$y, e$f, type = "diffmean")
nodesum(e$x, e$y, e$f, treef)
nodeplots(e$x, e$y, e$f, treef)
#
# build contrast boosting (CB) model on training data
#
z0 <- rep(0, length(d$y))
mdl <- modtrast(d$x, d$y, z0, type = "diffmean", niter = 200)
#
# test set error vs iterations
#
xval(e$x, e$y, z0, mdl)
#
# CB model predictions on test data
#
ecb <- predtrast(e$x, z0, mdl)
#
# CB average absolute error from truth (e$f) on test data
#
mean(abs(e$f - ecb))
```

```{r}
#
# compare model lack-of-fit curves
# LM=black, RF=red, CB=blue, truth=green
#
treelm <- contrast(e$x, e$y, elm, type = "diffmean", tree.size = 50)
treerf <- contrast(e$x, e$y, erf, type = "diffmean", tree.size = 50)
treeecb <- contrast(e$x, e$y, ecb, type = "diffmean", tree.size = 50)
treef <- contrast(e$x, e$y, e$f, type = "diffmean", tree.size = 50)
lofcurve(e$x, e$y, elm, treelm)
u <- lofcurve(e$x, e$y, erf, treerf, doplot = F)
lines(u$x, u$y, col = "red")
u <- lofcurve(e$x, e$y, ecb, treeecb, doplot = F)
lines(u$x, u$y, col = "blue")
u <- lofcurve(e$x, e$y, e$f, treef, doplot = F)
lines(u$x, u$y, col = "green")
#
# contrast p(y | x) with p(z | x) = N(0,1) on test data
#
z <- rnorm(length(e$y))
tree <- contrast(d$x, d$y, z)
nodesum(e$x, e$y, z, tree)
nodeplots(e$x, e$y, z, tree)
#
# distribution boosting
# estimate full p(y | x) starting with p(z | x) = N(0,1)
#
```

## Now things work, if I use your fortran flags

```{r}

mdld <- modtrast(d$x, d$y, z, niter = 200)
#
# test set discrepancies vs iterations
#
xval(e$x, e$y, z, mdld)
#
# transform z-values to estimated y-values
#
tg <- predtrast(e$x, z, mdld)
#
# contrast transformed p(g(z)|x) with p(y|x) on test data
#
tree <- contrast(e$x, e$y, tg)

nodesum(e$x, e$y, tg, tree)
```

```{r}
nodeplots(e$x, e$y, tg, tree)
```

```{r}
#
# compute predicted p(g(z)|x)(black)and compare with
# true p(y|x)(red)for first nine test set observations
#
p <- ((1:100) - .5) / 100
q <- qnorm(p)
kk <- 1
par(mfrow = c(3, 3))
for (k in kk:(kk + 8)) {
  t <- ydist(e$x[k, ], q, mdld)
  plot(t, p, type = "l", xlim = c(-5, 5))
  lines(t, 1 / (1 + exp((e$f[k] - t) / e$s[k])), col = "red")
}
```

## Session Info


```{r}
sessionInfo()
```
